{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pstats import SortKey\n",
    "import nest_asyncio, asyncio\n",
    "import cProfile, pstats, yaml, os\n",
    "from llama_index import Document\n",
    "from llama_index import SimpleDirectoryReader\n",
    "from llama_index.extractors import TitleExtractor\n",
    "from llama_index.embeddings import OpenAIEmbedding\n",
    "from llama_index.ingestion import IngestionPipeline\n",
    "from llama_index.text_splitter import SentenceSplitter\n",
    "from llama_index.embeddings import HuggingFaceEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = SimpleDirectoryReader(input_dir=\"./data\")\n",
    "documents = reader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the pipeline with transformations\n",
    "pipeline = IngestionPipeline(\n",
    "                            transformations=[\n",
    "                                            SentenceSplitter(\n",
    "                                                            chunk_size=1024, \n",
    "                                                            chunk_overlap=20\n",
    "                                                            ),\n",
    "                                            # TitleExtractor(),\n",
    "                                            HuggingFaceEmbedding(\n",
    "                                                                model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "                                                                device=\"mps\"\n",
    "                                            ),\n",
    "                                            ]\n",
    "                            )\n",
    "\n",
    "# since we'll be testing performance, using timeit and cProfile\n",
    "# we're going to disable cache\n",
    "pipeline.disable_cache = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit pipeline.run(documents=documents, num_workers=4)\n",
    "# nodes = pipeline.run(documents=documents, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Async Parallel Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loop = asyncio.get_event_loop()\n",
    "%timeit loop.run_until_complete(pipeline.arun(documents=documents, num_workers=4))\n",
    "# nodes = await pipeline.arun(documents=documents, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit pipeline.run(documents=documents)\n",
    "# nodes = pipeline.run(documents=documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Async on Main Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit loop.run_until_complete(pipeline.arun(documents=documents))\n",
    "# nodes = await pipeline.arun(documents=documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In Summary\n",
    "The results from the above experiments are re-shared below where each strategy is listed from fastest to slowest with this example dataset and pipeline.\n",
    "\n",
    "1. (Async, Parallel Processing)    : 20.3s\n",
    "2. (Async, No Parallel Processing) : 20.5s\n",
    "3. (Sync, Parallel Processing)     : 29s\n",
    "4. (Sync, No Parallel Processing)  : 1min 11s\n",
    "\n",
    "We can see that both cases that use Parallel Processing outperforms the Sync, No Parallel Processing (i.e., .run(num_workers=None)). Also, that at least for this case for Async tasks, there is little gains in using Parallel Processing. Perhaps for larger workloads and IngestionPipelines, using Async with Parallel Processing can lead to larger gains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaindex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
