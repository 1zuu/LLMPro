{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re, openai, yaml, os\n",
    "import http.client as httplib\n",
    "from llama_index.llms import AzureOpenAI\n",
    "from llama_index.schema import MetadataMode\n",
    "from llama_index.llm_predictor import LLMPredictor\n",
    "from llama_index import set_global_service_context\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from llama_index.embeddings import HuggingFaceEmbedding\n",
    "from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
    "from llama_index.node_parser import SimpleNodeParser, SentenceWindowNodeParser\n",
    "from llama_index.indices.postprocessor import MetadataReplacementPostProcessor\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.finetuning import (\n",
    "                                    generate_qa_embedding_pairs,\n",
    "                                    EmbeddingQAFinetuneDataset,\n",
    "                                    SentenceTransformersFinetuneEngine\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/1zuu/Desktop/LLM RESEARCH/LLMPro/cadentials.yaml') as f:\n",
    "    credentials = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "os.environ['AD_OPENAI_API_KEY'] = credentials['AD_OPENAI_API_KEY']\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = credentials['HUGGINGFACEHUB_API_TOKEN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'data/Camel Papers Train/'\n",
    "val_dir = 'data/Camel Papers Test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus(directory, verbose=False):\n",
    "    if verbose:\n",
    "        print(f\"Loading files in {directory}\")\n",
    "\n",
    "    reader = SimpleDirectoryReader(directory)\n",
    "    docs = reader.load_data()\n",
    "    if verbose:\n",
    "        print(f\"Loaded {len(docs)} docs\")\n",
    "\n",
    "    parser = SimpleNodeParser.from_defaults()\n",
    "    nodes = parser.get_nodes_from_documents(docs, show_progress=verbose)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Parsed {len(nodes)} nodes\")\n",
    "\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_llm = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "llm=AzureOpenAI(\n",
    "                deployment_name=credentials['AD_DEPLOYMENT_ID'],\n",
    "                model=credentials['AD_ENGINE'],\n",
    "                api_key=credentials['AD_OPENAI_API_KEY'],\n",
    "                api_version=credentials['AD_OPENAI_API_VERSION'],\n",
    "                azure_endpoint=credentials['AD_OPENAI_API_BASE']\n",
    "                )\n",
    "chat_llm = LLMPredictor(llm)\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "                                                embed_model=embedding_llm,\n",
    "                                                llm_predictor=chat_llm\n",
    "                                                )\n",
    "set_global_service_context(service_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files in data/Camel Papers Train/\n",
      "Loaded 91 docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 91/91 [00:00<00:00, 354.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 156 nodes\n",
      "Loading files in data/Camel Papers Test/\n",
      "Loaded 9 docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 9/9 [00:00<00:00, 245.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 17 nodes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_nodes = load_corpus(train_dir, verbose=True)\n",
    "val_nodes = load_corpus(val_dir, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156/156 [02:59<00:00,  1.15s/it]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = generate_qa_embedding_pairs(train_nodes, llm=llm)\n",
    "train_dataset.save_json(\"generated/train_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:20<00:00,  1.18s/it]\n"
     ]
    }
   ],
   "source": [
    "val_dataset = generate_qa_embedding_pairs(val_nodes, llm=llm)\n",
    "val_dataset.save_json(\"generated/val_dataset.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = EmbeddingQAFinetuneDataset.from_json(\"generated/train_dataset.json\")\n",
    "val_dataset = EmbeddingQAFinetuneDataset.from_json(\"generated/val_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_engine = SentenceTransformersFinetuneEngine(\n",
    "                                                    train_dataset,\n",
    "                                                    model_id=\"BAAI/bge-small-en-v1.5\",\n",
    "                                                    model_output_path=\"bge-small-finetuned\",\n",
    "                                                    val_dataset=val_dataset,\n",
    "                                                    epochs=2\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 31/31 [02:40<00:00,  5.19s/it]\n",
      "Iteration: 100%|██████████| 31/31 [02:39<00:00,  5.13s/it]\n",
      "Epoch: 100%|██████████| 2/2 [05:24<00:00, 162.05s/it]\n"
     ]
    }
   ],
   "source": [
    "finetune_engine.finetune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_embedding_llm = finetune_engine.get_finetuned_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuned Embedding Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_st(\n",
    "                dataset,\n",
    "                model_id,\n",
    "                name,\n",
    "                ):\n",
    "    \n",
    "    corpus = dataset.corpus\n",
    "    queries = dataset.queries\n",
    "    relevant_docs = dataset.relevant_docs\n",
    "\n",
    "    evaluator = InformationRetrievalEvaluator(queries, corpus, relevant_docs, name=name)\n",
    "    model = SentenceTransformer(model_id)\n",
    "    output_path = \"results/\"\n",
    "    Path(output_path).mkdir(exist_ok=True, parents=True)\n",
    "    return evaluator(model, output_path=output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7340686274509803"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_st(val_dataset, \"BAAI/bge-small-en-v1.5\", name=\"bge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8277777777777778"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_st(val_dataset, \"bge-small-finetuned\", name=\"finetuned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Retrieval Method: Sentence Window Retrieval\n",
    "\n",
    "Fine-tuning our embeddings is a powerful way to ensure we're better at retrieving the correct context - but we can go a step further and improve the way we actually look at context as well.\n",
    "\n",
    "In this demonstration, we'll be leveraging the idea of a SentenceWindowNodeParser and metadata replacement to take our retrieval to the next level.\n",
    "\n",
    "At a high level, what we're doing is straightforward:\n",
    "\n",
    "1. We parse our document into sentence-wise nodes.\n",
    "2. We find the most relevant sentence-wise nodes to our query.\n",
    "3. We add additional context based on a \"window\" around that base sentence-wise node.\n",
    "4. We use that enhanced context as context for our LLM!\n",
    "\n",
    "\n",
    "Let's look at this with a visual example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 684/684 [00:00<00:00, 269kB/s]\n",
      "model.safetensors: 100%|██████████| 133M/133M [00:15<00:00, 8.79MB/s] \n",
      "tokenizer_config.json: 100%|██████████| 366/366 [00:00<00:00, 148kB/s]\n",
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 1.85MB/s]\n",
      "tokenizer.json: 100%|██████████| 711k/711k [00:00<00:00, 5.75MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 125/125 [00:00<00:00, 94.0kB/s]\n"
     ]
    }
   ],
   "source": [
    "node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "                                                    window_size=6,\n",
    "                                                    window_metadata_key=\"window\",\n",
    "                                                    original_text_metadata_key=\"original_text\",\n",
    "                                                    )\n",
    "\n",
    "simple_node_parser = SimpleNodeParser.from_defaults() # simple node parser\n",
    "\n",
    "llm=AzureOpenAI(\n",
    "                deployment_name=credentials['AD_DEPLOYMENT_ID'],\n",
    "                model=credentials['AD_ENGINE'],\n",
    "                api_key=credentials['AD_OPENAI_API_KEY'],\n",
    "                api_version=credentials['AD_OPENAI_API_VERSION'],\n",
    "                azure_endpoint=credentials['AD_OPENAI_API_BASE']\n",
    "                )\n",
    "chat_llm = LLMPredictor(llm)\n",
    "\n",
    "# base Embeddings model\n",
    "embed_model_base = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en\")\n",
    "\n",
    "# fine-tuned Embeddings model\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"bge-small-finetuned\")\n",
    "\n",
    "# fine-tuned ServiceContext\n",
    "ctx = ServiceContext.from_defaults(\n",
    "                                    llm_predictor=chat_llm,\n",
    "                                    embed_model=embed_model,\n",
    "                                 )\n",
    "\n",
    "# base ServiceContext\n",
    "ctx_base = ServiceContext.from_defaults(\n",
    "                                        llm_predictor=chat_llm,\n",
    "                                        embed_model=embed_model_base\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create nodes using our `node_parser` and `simple_node_parser` after loading our documents found in the `TRAIN_FILES` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(train_dir).load_data()\n",
    "nodes = node_parser.get_nodes_from_documents(documents)\n",
    "nodes_base = simple_node_parser.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_index = VectorStoreIndex(nodes, service_context=ctx)\n",
    "sentence_index_base = VectorStoreIndex(nodes_base, service_context=ctx_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = sentence_index.as_query_engine(\n",
    "                                            similarity_top_k=3,\n",
    "                                            node_postprocessors=[\n",
    "                                                                MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n",
    "                                                                ],\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Genetics mechanisms controlling fiber traits in llamas and alpacas are not fully understood. However, a few genetic selection programs have been implemented in domestic camelids to improve fleece characteristics. The proteins that form the fiber are encoded by keratin genes (KRT) and keratin-associated proteins (KRTAP) which are expressed in a highly regulated manner during hair follicle growth. The presence of major genes affecting quantitative fiber traits such as fiber diameter, standard deviation of fiber diameter, variation coefficiency, and comfort factor in both Huacaya and Suri alpacas has been proposed by Perez-Cabal et al. based on segregation analysis.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_response = query_engine.query(\"How do camelid genetics influence wool quality?\")\n",
    "window_response.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
